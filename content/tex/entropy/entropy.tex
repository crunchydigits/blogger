\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{amsthm}


\newtheorem{definition}{Definition}


\title{Entropy and Information}
\author{Robert deCarvalho}
\date{}							% Activate to display a given date or no date
\begin{document}
\maketitle


\section{Introduction}
Entropy is one of those words that many people use, but few really understand.
This might be because the concept is invoked in seemingly very different
contexts.  For example, your physics teacher might have told you, ``The entropy
of the universe is increasing!'' Or perhaps you encountered the word while
reading about compression. ``This algorithm compressed our file to within
10\% of the entropy limit.''  Or maybe you vaguely remember entropy as something
your chemistry teacher told you about and maybe it had something to do with
reactions, but you can't quite remember.  Well, hopefully by the end of this
chapter you will have a decent grasp of what entropy is and how you can make
practical use of it.


\section{A Working Definition of Entropy}
Our quest to understand entropy will start with a simple definition.
\begin{definition}
    Entropy is the amount of information you do not know.
\end{definition}
That's a pretty short definition, but why would you possibly care about how much
information you don't know.  Shouldn't you care much more about you do know?
Also, what does it even mean to talk about an ``amount of informtion?'' You
probably have an intuitive sense of what ``information'' means, but how is it
quantified?  How could you tell if you knew presicely $2.73$ times more about a
topic today than you did yesterday?  As a guide to understanding how information
can be measured, we will create an imaginary library filled with books and
investigate how information plays into the way we orgainze our library.


\subsection{The Imaginary Library}
Imagine if you will, that I have a library in which I have collected exactly
$1000$ books. I have been very careful in selecting the titles that go into my
collection. I have only included books whos title's begin with the first ten
letters of the alphabet, A-J. As a matter of fact, there are exactly $100$ books
that begin with A, $100$ books that begin with B, and so on all the way through
the letter J.  I now have $1000$ books equally distributed among my $10$ bins.

MAYBE INSTEAD OF USING ALPHABETIZED BOOKS, USE CALL NUMBERS BECAUSE THIS
WILL MORE CLOSELY TIE INTO ENTROPY

\end{document}  
