\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}


\title{Understanding Entropy}
\author{Robert deCarvalho}
\date{}							% Activate to display a given date or no date
\begin{document}
\maketitle

\section{Introduction}
Entropy is one of those words that many people use, but few really understand.
This might be because the concept is invoked in seemingly very different
contexts.  For example, your physics teacher might have told you, ``The entropy
of the universe is increasing!'' Or perhaps you encountered the word while
reading about compression. ``This algorithm compressed our file to within
10\% of the entropy limit.''  Or maybe you vaguely remember entropy as something
your chemistry teacher told you about and maybe it had something to with
reactions, but you can't quite remember.  Well, hopefully by the end of this
chapter you will have a decent grasp of what entropy is and why it is useful
concept.


\section{Entropy is information you don't have}
That's right.  Entropy is the amount of information that you don't have.  Okay,
but what does that even mean?  Well, apparently, entropy is a kind of
information.  You probably a good working definition for what information is,
but would you know how to quantify it?  Would you be able to recognize when you
have precicely $2.7$ times more information about something than you did
yesterday?  Let me pose an artificial situation that I think nicely illustrates
a really useful way to think about quantifying information.

\subsection{The library of information}
Imagine if you will, that I have a library in which I have collected exactly
$1000$ books. I have been very careful in selecting the titles that go into my
collection. I have only included books whos title's begin with the first ten
letters of the alphabet, A-J. As a matter of fact, there are exactly $100$ books
that begin with A, $100$ books that begin with B, and so on all the way through
the letter J.  I now have $1000$ books equally distributed among my $10$ bins.

MAYBE INSTEAD OF USING ALPHABETIZED BOOKS, USE CALL NUMBERS BECAUSE THIS
WILL MORE CLOSELY TIE INTO ENTROPY

\end{document}  
